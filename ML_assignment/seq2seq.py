# -*- coding: utf-8 -*-
"""Machine_Translation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xiaCIgMqH4hI7jHhWoZbWQA4vI6DLF-B

# Machine Translation with Seq2Seq and Transformers
In this exercise you will implement a [Sequence to Sequence(Seq2Seq)](https://arxiv.org/abs/1703.03906) and a [Transformer](https://arxiv.org/pdf/1706.03762.pdf) model and use them to perform machine translation.

**A quick note: if you receive the following TypeError "super(type, obj): obj must be an instance or subtype of type", try re-importing that part or restarting your kernel and re-running all cells.** Once you have finished making changes to the model constuctor, you can avoid this issue by commenting out all of the model instantiations after the first (e.g. lines starting with "model = TransformerTranslator(*args, **kwargs)").
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import csv
import torch
from IPython.display import Image

# for auto-reloading external modules
# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython
# %load_ext autoreload
# %autoreload 2

"""# ** 1: Introduction**

## Multi30K: Multilingual English-German Image Descriptions

[Multi30K](https://github.com/multi30k/dataset) is a dataset for machine translation tasks. It is a multilingual corpus containing English sentences and their German translation. In total it contains 31014 sentences(29000 for training, 1014 for validation, and 1000 for testing).
As one example:

En: `Two young, White males are outside near many bushes.`

De: `Zwei junge weiße Männer sind im Freien in der Nähe vieler Büsche.`

You can read more info about the dataset [here](https://arxiv.org/abs/1605.00459). The following parts of this assignment will be based on this dataset.

## TorchText: A PyTorch Toolkit for Text Dataset and NLP Tasks
[TorchText](https://github.com/pytorch/text) is a PyTorch package that consists of data processing utilities and popular datasets for natural language. The key idea of TorchText is that datasets can be organized in *Field*, *TranslationDataset*, and *BucketIterator* classes. They serve to help with data splitting and loading, token encoding, sequence padding, etc. You don't need to know about how TorchText works in detail, but you might want to know about why those classes are needed and what operations are necessary for machine translation. This knowledge can be migrated to all sequential data modeling. In the following parts, we will provide you with some code to help you understand.

 You can refer to torchtext's documentation(v0.9.0) [here](https://pytorch.org/text/).

## Spacy
Spacy is package designed for tokenization in many languages. Tokenization is a process of splitting raw text data into lists of tokens that can be further processed. Since TorchText only provides tokenizer for English, we will be using Spacy for our assignment.


**Notice: For the following assignment, we strongly recommend you to work in a virtual python environment. We recommend Anaconda, a powerful environment control tool. You can download it [here](https://www.anaconda.com/products/individual)**.

## ** 1.1: Prerequisites**
Before you start this assignment, you need to have all required packages installed either on the terminal you are using, or in the virtual environment. Please make sure you have the following package installed:

`PyTorch, TorchText, Spacy, Tqdm, Numpy`

You can first check using either `pip freeze` in terminal or `conda list` in conda environment. Then run the following code block to make sure they can be imported.
"""

# Just run this block. Please do not modify the following code.
import math
import time

# Pytorch package
import torch
import torch.nn as nn
import torch.optim as optim

# Torchtest package
from torchtext.datasets import Multi30k
from torchtext.data import Field, BucketIterator

# Tqdm progress bar
from tqdm import tqdm_notebook, tqdm

# Code provide to you for training and evaluation
from utils import train, evaluate, set_seed_nb, unit_test_values, deterministic_init

"""Once you properly import the above packages, you can proceed to download Spacy English and German tokenizers by running the following command in your **terminal**. They will take some time.

`python -m spacy download en`

`python -m spacy download de`

Now lets check your GPU availability and load some sanity checkers. By default you should be using your gpu for this assignment if you have one available.
"""

# Check device availability
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print("You are using device: %s" % device)

# load checkers
d1 = torch.load('./data/d1.pt')
d2 = torch.load('./data/d2.pt')
d3 = torch.load('./data/d3.pt')
d4 = torch.load('./data/d4.pt')

"""## **1.2: Preprocess Data**
With TorchText and Spacy tokenizers ready, you can now prepare the data using *TorchText* objects. Just run the following code blocks. Read the comment and try to understand what they are for.
"""

# You don't need to modify any code in this block

# Define the maximum length of the sentence. Shorter sentences will be padded to that length and longer sentences will be croped. Given that the average length of the sentence in the corpus is around 13, we can set it to 20
MAX_LEN = 20
Multi30k.urls = ['https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/training.tar.gz',
'https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/validation.tar.gz',
'https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/mmt_task1_test2016.tar.gz']

# Define the source and target language
SRC = Field(tokenize = "spacy",
            tokenizer_language="de",
            init_token = '<sos>',
            eos_token = '<eos>',
            fix_length = MAX_LEN,
            lower = True)

TRG = Field(tokenize = "spacy",
            tokenizer_language="en",
            init_token = '<sos>',
            eos_token = '<eos>',
            fix_length = MAX_LEN,
            lower = True)

# Download and split the data. It should take some time
train_data, valid_data, test_data = Multi30k.splits(exts = ('.de', '.en'),
                                                    fields = (SRC, TRG),root='data')

# Define Batchsize
BATCH_SIZE = 128

# Build the vocabulary associated with each language
SRC.build_vocab(train_data, min_freq = 2)
TRG.build_vocab(train_data, min_freq = 2)

# Get the padding index to be ignored later in loss calculation
PAD_IDX = TRG.vocab.stoi['<pad>']

# Get data-loaders using BucketIterator
train_loader, valid_loader, test_loader = BucketIterator.splits(
    (train_data, valid_data, test_data),
    batch_size = BATCH_SIZE, device = device)

# Get the input and the output sizes for model
input_size = len(SRC.vocab)
output_size = len(TRG.vocab)

"""# **2: Implement Vanilla RNN and LSTM**
In this section, you will need to implement a Vanilla RNN and an LSTM unit using PyTorch Linear layers and nn.Parameter. This is designed to help you to understand how they work behind the scene. The code you will be working with is in *LSTM.py* and *RNN.py* under *naive* folder. Please refer to instructions among this notebook and those files.

## **2.1: Implement an RNN Unit**
In this section you will be using PyTorch Linear layers and activations to implement a vanilla RNN unit. You are only implementing an RNN cell unit over one time step! The test case reflects this by having only one sequence. Please refer to the following structure and complete the code in RNN.py:
"""

Image (filename="imgs/RNN.png", retina=True)

"""Run the following block to check your implementation"""

from models.naive.RNN import VanillaRNN

x1,x2 = (1,4), (-1,2)
h1,h2 = (-1,2,0,4), (0,1,3,-1)
batch = 4
x = torch.FloatTensor(np.linspace(x1,x2,batch))
h = torch.FloatTensor(np.linspace(h1,h2,batch))
rnn = VanillaRNN(x.shape[-1], h.shape[-1], 3)

expected_out, expected_hidden = unit_test_values('rnn')

deterministic_init(rnn)
out, hidden = rnn.forward(x,h)

if out is not None:
    print("RNN results")
    print('Close to out: ', expected_out.allclose(out, atol=1e-4))
    print('Close to hidden: ', expected_hidden.allclose(hidden, atol=1e-4))
else:
    print("NOT IMPLEMENTED")

"""## **2.2: Implement an LSTM Unit**
In this section you will be using PyTorch nn.Parameter and activations to implement an LSTM unit. You can simply translate the following equations using nn.Parameter and PyTorch activation functions to build an LSTM from scratch:
\begin{array}{ll} \\
    i_t = \sigma(W_{ii} x_t + b_{ii} + W_{hi} h_{t-1} + b_{hi}) \\
    f_t = \sigma(W_{if} x_t + b_{if} + W_{hf} h_{t-1} + b_{hf}) \\
    g_t = \tanh(W_{ig} x_t + b_{ig} + W_{hg} h_{t-1} + b_{hg}) \\
    o_t = \sigma(W_{io} x_t + b_{io} + W_{ho} h_{t-1} + b_{ho}) \\
    c_t = f_t \odot c_{t-1} + i_t \odot g_t \\
    h_t = o_t \odot \tanh(c_t) \\
\end{array}

Here's a great visualization of the above equation from [Colah's blog](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) to help you understand LSTM unit. You can also read more about it from that blog.
"""

Image (filename="imgs/lstm.png", retina=True)

"""If you want to see nn.Parameter in example, check out this [tutorial](https://pytorch.org/tutorials/beginner/nn_tutorial.html) from PyTorch. Run the following block to check your implementation.

Note that in this case we are implementing a full loop with LSTM, iterating over each time step. The test cases reflect this as there are multiple sequences.
"""

from models.naive.LSTM import LSTM

set_seed_nb()
x1,x2 = np.mgrid[-1:3:3j, -1:4:2j]
h1,h2 = np.mgrid[-2:2:3j, 1:3:4j]
batch = 4
x = torch.FloatTensor(np.linspace(x1,x2,batch))
h = torch.FloatTensor(np.linspace(h1,h2,batch))

expected_ht, expected_ct = unit_test_values('lstm')

lstm = LSTM(x.shape[-1], h.shape[-1])
deterministic_init(lstm)
h_t, c_t = lstm.forward(x)

if h_t is not None:
    print("LSTM results")
    print('Close to h_t: ', expected_ht.allclose(h_t, atol=1e-4))
    print('Close to c_t; ', expected_ct.allclose(c_t, atol=1e-4))
else:
    print("NOT IMPLEMENTED")

"""# **3: Train a Seq2Seq Model**
In this section, you will be working on implementing a simple Seq2Seq model. You will first implement an Encoder and a Decoder, and then join them together with a Seq2Seq architecture. You will need to complete the code in *Decoder.py*, *Encoder.py*, and *Seq2Seq.py* under *seq2seq* folder. Please refer to the instructions in those files.

## **3.1: Implement the Encoder**
In this section you will be implementing an RNN/LSTM based encoder to model English texts. Please refer to the instructions in *seq2seq/Encoder.py*. Run the following block to check your implementation.
"""


from models.seq2seq.Encoder import Encoder

set_seed_nb()
i, n, h = 10, 4, 2

encoder = Encoder(i, n, h, h)
x_array = np.random.rand(5,1) * 10
x = torch.LongTensor(x_array)
out, hidden = encoder.forward(x)

expected_out, expected_hidden = unit_test_values('encoder')
print("Encoder results")
print('Close to out: ', expected_out.allclose(out, atol=1e-4))
print('Close to hidden: ', expected_hidden.allclose(hidden, atol=1e-4))

import torch
import torch.nn as nn

# class RNNEncoder(nn.Module):
#     def __init__(self, input_size, hidden_size, num_layers, model_type="RNN"):
#         super(RNNEncoder, self).__init__()

#         self.input_size = input_size
#         self.hidden_size = hidden_size
#         self.num_layers = num_layers
#         self.model_type = model_type

#         # Initialize the RNN layer based on the model type
#         if model_type == "RNN":
#             self.rnn = nn.RNN(input_size, hidden_size, num_layers=num_layers, batch_first=True)
#         elif model_type == "LSTM":
#             self.rnn = nn.LSTM(input_size, hidden_size, num_layers=num_layers, batch_first=True)

#     def forward(self, x):
#         # Forward pass through the RNN layer
#         output, hidden = self.rnn(x)

#         return output, hidden

# # Example usage
# input_size = 10
# hidden_size = 2
# num_layers = 1
# model_type = "RNN"  # or "LSTM"

# # Initialize the encoder RNN module
# encoder_rnn = RNNEncoder(input_size, hidden_size, num_layers, model_type)

# # Generate some dummy input
# batch_size = 32
# sequence_length = 10
# input_tensor = torch.randn(batch_size, sequence_length, input_size)

# # Forward pass through the encoder RNN module
# output, hidden = encoder_rnn(input_tensor)

# # Output shape: (batch_size, sequence_length, hidden_size)
# print("Output shape:", output.shape)
# # Hidden shape: (num_layers, batch_size, hidden_size)
# print("Hidden shape:", hidden[0].shape)  # For RNN, LSTM

"""## **3.2: Implement the Decoder**
In this section you will be implementing an RNN/LSTM based decoder to model German texts. Please refer to the instructions in *seq2seq/Decoder.py*. Run the following block to check your implementation.
"""

from models.seq2seq.Decoder import Decoder

set_seed_nb()
i, n, h =  10, 2, 2
decoder = Decoder(h, n, n, i)
x_array = np.random.rand(5, 1) * 10
x = torch.LongTensor(x_array)
#print(f"{x = }")
#print(f"{enc_hidden = }, {enc_hidden = }")
enc_hidden = torch.FloatTensor([[[0.4912, -0.6078],
                                [0.4912, -0.6078],
                                [0.4985, -0.6658],
                                [0.4932, -0.6242],
                                [0.4880, -0.7841]]]) #np.random.rand(32,2) * 10 # unit_test_values('encoder')
out, hidden = decoder.forward(x,enc_hidden)

expected_out, expected_hidden = unit_test_values('decoder')
# print(f"{hidden = }, {hidden.shape = }")
print("Decoder results")
print('Close to out: ', expected_out.allclose(out, atol=1e-4))
print('Close to hidden: ', expected_hidden.allclose(hidden, atol=1e-4))

"""## **3.3: Implement the Seq2Seq**
In this section you will be implementing the Seq2Seq model that utilizes the Encoder and Decoder you implemented. Please refer to the instructions in *seq2seq/Seq2Seq.py*. Run the following block to check your implementation.
"""


import torch
import torch.nn as nn

# Define the parameters
input_size = 32
hidden_size = 64
num_layers = 1
batch_size = 2  # Change the batch size to 2

# Define the RNN
rnn = nn.RNN(input_size, hidden_size, num_layers)

# Initialize the hidden state with the desired batch size
# Make sure to pass the appropriate device if using GPU
hidden = torch.zeros(num_layers, batch_size, hidden_size)
# print(f"{hidden = }, {hidden.shape = }")
# Example input tensor
input_tensor = torch.randn(1, batch_size, input_size)

# Forward pass
output, hidden = rnn(input_tensor, hidden)
# print(f"{hidden = }, {hidden.shape = }")

from models.seq2seq.Seq2Seq import Seq2Seq

set_seed_nb()
embedding_size = 32
hidden_size = 32
input_size = 8
output_size = 8
batch, seq = 1, 2

encoder = Encoder(input_size, embedding_size, hidden_size, hidden_size)
decoder = Decoder(embedding_size, hidden_size, hidden_size, output_size)

seq2seq = Seq2Seq(encoder, decoder, 'cpu')
x_array = np.random.rand(batch, seq) * 10
x = torch.LongTensor(x_array)
out = seq2seq.forward(x)

expected_out = unit_test_values('seq2seq')
print('Close to out: ', expected_out.allclose(out, atol=1e-4))

"""## **3.4: Train your Seq2Seq model**
Now it's time to combine what we have and train a Seq2Seq translator. We provided you with some training code and you can simply run them to see how your translator works. If you implemented everything correctly, you should see some meaningful translation in the output. You can modify the hyperparameters to improve the results. You can also tune the BATCH_SIZE in section 1.2.
"""

# Hyperparameters. You are welcome to modify these
encoder_emb_size = 32
encoder_hidden_size = 64
encoder_dropout = 0.2

decoder_emb_size = 32
decoder_hidden_size = 64
decoder_dropout = 0.2

learning_rate = 1e-3
model_type = "LSTM"

EPOCHS = 10

#input size and output size
input_size = len(SRC.vocab)
print(input_size)
output_size = len(TRG.vocab)
print(output_size)

# Declare models, optimizer, and loss function
encoder = Encoder(input_size, encoder_emb_size, encoder_hidden_size, decoder_hidden_size, dropout = encoder_dropout, model_type = model_type)
decoder = Decoder(decoder_emb_size, encoder_hidden_size, encoder_hidden_size, output_size, dropout = decoder_dropout, model_type = model_type)
seq2seq_model = Seq2Seq(encoder, decoder, device)

# optimizer = optim.Adam(seq2seq_model.parameters(), lr = learning_rate)
# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer)
# criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)

### Seq2seq training ###
# for epoch_idx in range(EPOCHS):
#     torch.cuda.empty_cache()
#     print("-----------------------------------")
#     print("Epoch %d" % (epoch_idx+1))
#     print("-----------------------------------")

#     train_loss, avg_train_loss = train(seq2seq_model, train_loader, optimizer, criterion)
#     scheduler.step(train_loss)

#     val_loss, avg_val_loss = evaluate(seq2seq_model, valid_loader, criterion)

#     print("Training Loss: %.4f. Validation Loss: %.4f. " % (avg_train_loss, avg_val_loss))
#     print("Training Perplexity: %.4f. Validation Perplexity: %.4f. " % (np.exp(avg_train_loss), np.exp(avg_val_loss)))


# Batch size
batch_sizes = [64] # 64, 128, 256, 512]

# Hyperparameters
LRS = [1e-3] #, 1e-3, 1e-4, 5e-4, 5e-5, 3e-3 1e-1]
num_epochs = [20, 50, 100, 200]
min_loss = np.inf
model_loss = np.inf
best_model = {}
# Model
for BATCH_SIZE in batch_sizes:
    for learning_rate in LRS:
        for EPOCHS in num_epochs:
            torch.cuda.empty_cache()
            print("Num Epochs: ", EPOCHS)
            train_loader, valid_loader, test_loader = BucketIterator.splits(
                (train_data, valid_data, test_data),
                batch_size = BATCH_SIZE, device = device)
            seq2seq_model = Seq2Seq(encoder, decoder, device)
            # optimizer = optim.Adam(model.parameters(), lr = learning_rate)
            optimizer = torch.optim.Adam(seq2seq_model.parameters(), lr=learning_rate)
            scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer)
            criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)

            for epoch_idx in range(EPOCHS):
                # print("-----------------------------------")
                # print("Epoch %d" % (epoch_idx+1))
                # print("-----------------------------------")

                train_loss, avg_train_loss = train(seq2seq_model, train_loader, optimizer, criterion)
                scheduler.step(train_loss)

                val_loss, avg_val_loss = evaluate(seq2seq_model, valid_loader, criterion)
                if avg_val_loss < min_loss:
                    min_loss = avg_val_loss
                    best_model["batch_size"] = BATCH_SIZE
                    best_model["epochs"] = EPOCHS
                    best_model["lr"] = learning_rate
            val_loss, avg_val_loss = evaluate(seq2seq_model, valid_loader, criterion)
            if avg_val_loss < model_loss:
                model_loss = avg_val_loss
                torch.save({
                    'model_state_dict': seq2seq_model.state_dict(),
                    'epoch': EPOCHS,
                    'lr': learning_rate,
                    'batch_size': BATCH_SIZE,
                    'loss': avg_val_loss,
                    'optimizer_state_dict': optimizer.state_dict(),
                    }, "/home/rafayel.veziryan/ml-assignment/ML_assignment/Best_models_parameters/seq2seq_" + str(BATCH_SIZE) + "_" + str(learning_rate) + ".pt")
                # print("Training Loss: %.4f. Validation Loss: %.4f. " % (avg_train_loss, avg_val_loss))
                # print("Training Perplexity: %.4f. Validation Perplexity: %.4f. " % (np.exp(avg_train_loss), np.exp(avg_val_loss)))

with open("./Best_models_parameters/seq2seq_best_parameters_" + str(BATCH_SIZE) + "_" + str(learning_rate) + ".txt", mode="wt") as f:
  f.write(f"Best model parameters: {best_model}")
print("Best model parameters: ", best_model)
